{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ec6c1e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nb\n",
    "from copy import deepcopy\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib import colors\n",
    "from mne import read_epochs, pick_types\n",
    "from mne.time_frequency import psd_array_welch\n",
    "from scipy.interpolate import interp1d\n",
    "import new_files\n",
    "import os.path as op\n",
    "from os import sep\n",
    "import trimesh\n",
    "import networkx as nx\n",
    "import open3d as o3d\n",
    "from tools import compute_rel_power, get_crossover, detect_crossing_points, data_to_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "84e8e72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_np_arr(ar1, ar2):\n",
    "    if ar1.shape == ar2.shape:\n",
    "        return np.all(ar1 == ar2)\n",
    "    elif ar1.shape != ar2.shape:\n",
    "        return False\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8605e205",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_search = new_files.Files()\n",
    "dataset_path = \"/home/common/bonaiuto/multiburst/derivatives/processed\"\n",
    "img_path = \"/scratch/poster_visualisations\"\n",
    "all_jsons = dir_search.get_files(dataset_path,\"*.json\", strings=[\"info\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87985d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_types = {\n",
    "    \"visual\": [np.linspace(-0.2, 0.8, num=601), [0.0, 0.2], -0.01],\n",
    "    \"motor\": [np.linspace(-0.5, 0.5, num=601), [-0.2, 0.2], -0.2]\n",
    "}\n",
    "\n",
    "crop_info = {\n",
    "    \"visual\": (-0.2, 0.8),\n",
    "    \"motor\": (-0.5, 0.5)\n",
    "}\n",
    "\n",
    "flims = [0.1,125] # freq limits for psd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "972c2ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sub_ix in range(6):\n",
    "    json_file = all_jsons[sub_ix]\n",
    "    with open(json_file) as pipeline_file:\n",
    "        info = json.load(pipeline_file)\n",
    "\n",
    "    cortical_thickness = np.load(info[\"cortical_thickness_path\"])\n",
    "    atlas_labels = np.load(info[\"atlas_labels_path\"])\n",
    "    atlas_colours = np.load(info[\"atlas_colors_path\"])\n",
    "    atlas = pd.read_csv(info[\"atlas\"])\n",
    "\n",
    "    fif_mu = list(zip(info[\"sensor_epochs_paths\"], info[\"MU_paths\"]))\n",
    "\n",
    "    # SUBJ NAMES\n",
    "    for file_no in range(4):\n",
    "        fif, MU = fif_mu[file_no]\n",
    "        epoch_type = [i for i in epoch_types.keys() if i in fif][0]\n",
    "        subject = fif.split(sep)[-3]\n",
    "\n",
    "        # CSD CALC with FIF MU\n",
    "        core_name = fif.split(sep)[-1].split(\"_\")[-1].split(\".\")[0]\n",
    "        epo_type = [i for i in crop_info.keys() if i in fif][0]\n",
    "        fif = read_epochs(fif, verbose=False)\n",
    "        fif = fif.pick_types(meg=True, ref_meg=False, misc=False, verbose=False)\n",
    "        fif_all = fif.get_data()\n",
    "        fif = fif.crop(tmin=crop_info[epo_type][0], tmax=crop_info[epo_type][1])\n",
    "        sfreq = fif.info[\"sfreq\"]\n",
    "        fif_times = fif.times\n",
    "        fif = fif.get_data()\n",
    "        fif = np.mean(fif, axis=0) # if no split on conditions\n",
    "\n",
    "        new_mu_path = MU.split(\".\")[0] + \".npy\"\n",
    "        if not op.exists(new_mu_path):\n",
    "            MU = pd.read_csv(MU, sep=\"\\t\", header=None).to_numpy()\n",
    "            np.save(new_mu_path, MU)\n",
    "        elif op.exists(new_mu_path):\n",
    "            MU = np.load(new_mu_path)\n",
    "\n",
    "        MU = np.split(MU, info[\"n_surf\"], axis=0)\n",
    "        layer_shape = MU[0].shape[0]\n",
    "\n",
    "        src = []\n",
    "        for i in range(layer_shape):\n",
    "            vertex_layers = np.array([mx[i] for mx in MU])\n",
    "            vertex_source = np.dot(fif.T, vertex_layers.T).T\n",
    "            src.append(vertex_source) \n",
    "        src = np.array(src)\n",
    "\n",
    "        metric = np.log10(np.var(np.mean(src, axis=1), axis=1))\n",
    "        # metric = np.log10(np.max(np.abs(np.mean(src, axis=1)), axis=1))\n",
    "        brain = nb.load(info[\"pial_ds_nodeep_inflated\"])\n",
    "        vertices, faces = brain.agg_data()\n",
    "        mesh = trimesh.Trimesh(vertices=vertices, faces=faces, process=False, validate=False)\n",
    "        vx_neighbours = mesh.vertex_neighbors\n",
    "        map_perc = metric >= np.percentile(metric, 99)\n",
    "        vx_neighbours = [np.array(i) for i in vx_neighbours]\n",
    "        removed_mesh = np.arange(vertices.shape[0])[map_perc]\n",
    "        removed_mesh_neighbours = [np.intersect1d(vx_neighbours[i], removed_mesh) for i in removed_mesh]\n",
    "        dict_mesh = {i[0]: i[1] for i in list(zip(removed_mesh, removed_mesh_neighbours)) if len(i[1]) > 0}\n",
    "        vv_graph = nx.from_dict_of_lists(dict_mesh)\n",
    "        all_clusters = {i: np.sort(np.array(list(nx.node_connected_component(vv_graph, i)))) for i in list(dict_mesh)}\n",
    "        unique_clusters = {}\n",
    "        for val in list(all_clusters.values()):\n",
    "            if unique_clusters.get(tuple(val)) == None:\n",
    "                keys = [i for i in list(all_clusters.keys()) if comp_np_arr(all_clusters[i], val)]\n",
    "                unique_clusters[tuple(val)] = len(keys)\n",
    "\n",
    "        unique_clusters = [np.array(i) for i in list(unique_clusters.keys())]\n",
    "        unique_clusters = [i for i in unique_clusters if i.shape[0] > 4]\n",
    "        cluster_vertices = np.hstack(unique_clusters)\n",
    "        data = {}\n",
    "        data[\"json\"] = json_file\n",
    "        data[\"log_variance\"] = log_vpv\n",
    "        data[\"atlas_labels\"] = np.array([i.decode(\"utf=8\") for i in atlas_labels])\n",
    "        data[\"clusters\"] = unique_clusters\n",
    "\n",
    "        # loop over the cluster vertices\n",
    "        vertex_results = {}\n",
    "        for vx in cluster_vertices:\n",
    "            signal = src[vx]\n",
    "\n",
    "\n",
    "            vertex_source = []\n",
    "            for trial in fif_all:\n",
    "                layered = np.array([np.dot(trial.T, MU[i][vx]) for i in range(info[\"layers\"])])\n",
    "                vertex_source.append(np.array(layered))\n",
    "            vertex_source = np.array(vertex_source)\n",
    "\n",
    "            winsize = int(sfreq)\n",
    "            overlap = int(winsize/2)\n",
    "            psd, freqs = psd_array_welch(\n",
    "                vertex_source, sfreq, fmin=flims[0], \n",
    "                fmax=flims[1], n_fft=2000, \n",
    "                n_overlap=overlap, n_per_seg=winsize,\n",
    "                window=\"hann\", verbose=False, n_jobs=1\n",
    "            )\n",
    "            mean_psd = np.nanmean(psd,axis=0)\n",
    "\n",
    "            vertex_results[(vx, \"signal\")] = signal\n",
    "            vertex_results[(vx, \"mean_psd\")] = mean_psd\n",
    "            vertex_results[\"freqs\"] = freqs\n",
    "\n",
    "        data[\"vertex_mean_signal_psd\"] = vertex_results\n",
    "\n",
    "        filename = \"raw_preproc_{}.pickle\".format(core_name)\n",
    "        out_path = op.join(img_path, filename)\n",
    "        with open(out_path, \"wb\") as handle:\n",
    "            pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "59a7d747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['L_A4_ROI', 'L_A4_ROI', 'L_A4_ROI', ..., 'R_PFop_ROI',\n",
       "       'R_PFop_ROI', 'R_PFop_ROI'], dtype='<U12')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atlas_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5f2172",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
